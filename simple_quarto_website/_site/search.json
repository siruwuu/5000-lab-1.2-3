[
  {
    "objectID": "slides/slides.html#section",
    "href": "slides/slides.html#section",
    "title": "Bayes‚Äô Theorem",
    "section": "",
    "text": "Bayes‚Äô theorem (alternatively Bayes‚Äô law or Bayes‚Äô rule, after Thomas Bayes) gives a mathematical rule for inverting conditional probabilities, allowing us to find the probability of a cause given its effect."
  },
  {
    "objectID": "slides/slides.html#section-1",
    "href": "slides/slides.html#section-1",
    "title": "Bayes‚Äô Theorem",
    "section": "",
    "text": "History\nBayes‚Äô theorem is named after the Reverend Thomas Bayes, also a statistician and philosopher. Bayes used conditional probability to provide an algorithm that uses evidence to calculate limits on an unknown parameter. Bayes studied how to compute a distribution for the probability parameter of a binomial distribution (in modern terminology). (Swinburne 2004) About 200 years later, Sir Harold Jeffreys1 put Bayes‚Äôs algorithm and Laplace‚Äôs formulation on an axiomatic basis, writing in a 1973 book that:\n\n‚ÄúBayes‚Äô theorem is to the theory of probability what the Pythagorean theorem is to geometry.‚Äù"
  },
  {
    "objectID": "slides/slides.html#statement-of-theorem",
    "href": "slides/slides.html#statement-of-theorem",
    "title": "Bayes‚Äô Theorem",
    "section": "Statement of Theorem",
    "text": "Statement of Theorem\nBayes‚Äô theorem is stated mathematically as the following equation:\n\\[\nP(A|B) = \\frac{P(B|A) P(A)}{P(B)}\n\\] where \\(ùê¥\\) and \\(ùêµ\\) are events and \\(P(B) \\not = 0\\)."
  },
  {
    "objectID": "slides/slides.html#section-2",
    "href": "slides/slides.html#section-2",
    "title": "Bayes‚Äô Theorem",
    "section": "",
    "text": "\\(P(A|B)\\) is a conditional probability: the probability of event \\(A\\) occurring given that \\(B\\) is true. It is also called the posterior probability of \\(A\\) given \\(B\\).\n\\(P(B|A)\\) is also a conditional probability: the probability of event \\(B\\) occurring given that \\(A\\) is true. It can also be interpreted as the likelihood of \\(A\\) given a fixed \\(B\\) because \\(P(B|A) = L(A|B)\\).\n\\(P(A)\\) and \\(P(B)\\) are the probabilities of observing \\(A\\) and \\(B\\) respectively without any given conditions; they are known as the prior probability and marginal probability."
  },
  {
    "objectID": "slides/slides.html#h",
    "href": "slides/slides.html#h",
    "title": "Bayes‚Äô Theorem",
    "section": "h",
    "text": "h\n\n\nCode\nimport matplotlib.pyplot as plt\n\nx = [1, 2, 3, 4, 5]\ny = [1, 4, 9, 16, 25]\n\nplt.plot(x, y)\nplt.show()"
  },
  {
    "objectID": "slides/slides.html#reference",
    "href": "slides/slides.html#reference",
    "title": "Bayes‚Äô Theorem",
    "section": "Reference",
    "text": "Reference\n\n\n\n\n\n\n\n\nSwinburne, Richard. 2004. ‚ÄúBayes‚Äô Theorem.‚Äù Revue Philosophique de La France Et de l 194 (2)."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "about.html#phasellus-vulputate-ad-ligula-torquent-volutpat-vitae.",
    "href": "about.html#phasellus-vulputate-ad-ligula-torquent-volutpat-vitae.",
    "title": "About",
    "section": "Phasellus vulputate ad ligula torquent volutpat vitae.",
    "text": "Phasellus vulputate ad ligula torquent volutpat vitae.\nTorquent massa eu magna magna ornare convallis semper erat sollicitudin. Elit ridiculus ligula mi nam vivamus orci. Mattis pulvinar suspendisse risus quam facilisi congue condimentum montes. Nullam ipsum convallis ex taciti consectetur vel posuere. Diam aliquam neque arcu luctus gravida consequat ridiculus risus. Torquent ultrices lectus mus cras iaculis volutpat potenti in.\n\nVolutpat sociosqu vitae ad dui justo lacinia.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nxpoints = np.array([1, 8])\nypoints = np.array([3, 10])\n\nplt.plot(xpoints, ypoints)\nplt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayes‚Äô Theorem",
    "section": "",
    "text": "Bayes‚Äô theorem (alternatively Bayes‚Äô law or Bayes‚Äô rule, after Thomas Bayes) gives a mathematical rule for inverting conditional probabilities, allowing us to find the probability of a cause given its effect. For example, if the risk of developing health problems is known to increase with age, Bayes‚Äô theorem allows the risk to an individual of a known age to be assessed more accurately by conditioning it relative to their age, rather than assuming that the individual is typical of the population as a whole.\nBased on Bayes law both the prevalence of a disease in a given population and the error rate of an infectious disease test have to be taken into account to evaluate the meaning of a positive test result correctly and avoid the base-rate fallacy. (Koch and Koch 1990)"
  },
  {
    "objectID": "index.html#history",
    "href": "index.html#history",
    "title": "Bayes‚Äô Theorem",
    "section": "History",
    "text": "History\n\n\nBayes‚Äô theorem is named after the Reverend Thomas Bayes (/be…™z/), also a statistician and philosopher.\nBayes used conditional probability to provide an algorithm that uses evidence to calculate limits on an unknown parameter. His work was published in 1763 as An Essay Towards Solving a Problem in the Doctrine of Chances.\nBayes studied how to compute a distribution for the probability parameter of a binomial distribution (in modern terminology).\n\nIndependently of Bayes, Pierre-Simon Laplace in 1774, and later in his 1812 Th√©orie analytique des probabilit√©s, used conditional probability to formulate the relation of an updated posterior probability from a prior probability, given evidence.\nHe reproduced and extended Bayes‚Äôs results in 1774, apparently unaware of Bayes‚Äôs work. The Bayesian interpretation of probability was developed mainly by Laplace. (Swinburne 2004)\n\n\nAbout 200 years later, Sir Harold Jeffreys1 put Bayes‚Äôs algorithm and Laplace‚Äôs formulation on an axiomatic basis, writing in a 1973 book that\n\n‚ÄúBayes‚Äô theorem is to the theory of probability what the Pythagorean theorem is to geometry.‚Äù"
  },
  {
    "objectID": "index.html#statement-of-theorem",
    "href": "index.html#statement-of-theorem",
    "title": "Bayes‚Äô Theorem",
    "section": "Statement of Theorem",
    "text": "Statement of Theorem\nBayes‚Äô theorem is stated mathematically as the following equation:\n\\[\nP(A|B) = \\frac{P(B|A) P(A)}{P(B)}\n\\] where \\(ùê¥\\) and \\(ùêµ\\) are events and \\(P(B) \\not = 0\\).\n\n\\(P(A|B)\\) is a conditional probability2: the probability of event \\(A\\) occurring given that \\(B\\) is true. It is also called the posterior probability3 of \\(A\\) given \\(B\\).\n\\(P(B|A)\\) is also a conditional probability: the probability of event \\(B\\) occurring given that \\(A\\) is true. It can also be interpreted as the likelihood4 of \\(A\\) given a fixed \\(B\\) because \\(P(B|A) = L(A|B)\\).\n\\(P(A)\\) and \\(P(B)\\) are the probabilities of observing \\(A\\) and \\(B\\) respectively without any given conditions; they are known as the prior probability5 and marginal probability6.\n\n\n\nBeing General\n\n\n\n\nB\nnotB\n\n\n\n\n\n\nA\ns\nt\ns + t\n\n\n\nnotA\nu\nv\nu + v\n\n\n\n\ns + u\nt + v\ns + t + u + v\n\n\n\n\n\n\n\n\n\n\n\n\n\n    graph TD \n    A[Christmas] --&gt;|Get money|B(Go shopping) \n    B --&gt; C{Let me think} \n    C --&gt;|One| D[Laptop] \n    C --&gt;|Two| E[iPhone] \n    C --&gt;|Three| F[fa:fa-car Car]"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Bayes‚Äô Theorem",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSir Harold Jeffreys (22 April 1891 ‚Äì 18 March 1989) was a British geophysicist who made significant contributions to mathematics and statistics. His book, Theory of Probability, which was first published in 1939, played an important role in the revival of the objective Bayesian view of probability.‚Ü©Ô∏é\nConditional probability is a measure of the probability of an event occurring, given that another event (by assumption, presumption, assertion or evidence) is already known to have occurred.‚Ü©Ô∏é\nPosterior probability is a type of conditional probability that results from updating the prior probability with information summarized by the likelihood via an application of Bayes‚Äô rule.‚Ü©Ô∏é\nLikelihood function (often simply called the likelihood) measures how well a statistical model explains observed data by calculating the probability of seeing that data under different parameter values of the model.‚Ü©Ô∏é\nPrior probability distribution of an uncertain quantity, often simply called the prior, is its assumed probability distribution before some evidence is taken into account.‚Ü©Ô∏é\nMarginal distribution of a subset of a collection of random variables is the probability distribution of the variables contained in the subset.‚Ü©Ô∏é"
  },
  {
    "objectID": "slides/slides.html#section-4",
    "href": "slides/slides.html#section-4",
    "title": "Bayes‚Äô Theorem",
    "section": "",
    "text": "Code\nimport matplotlib.pyplot as plt\n\nx = [1, 2, 3, 4, 5]\ny = [1, 4, 9, 16, 25]\n\nplt.plot(x, y)\nplt.show()"
  }
]